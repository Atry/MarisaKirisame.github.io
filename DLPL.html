<html>
<head>
<meta charset="utf-8" />
<title>Deep Learning, from a Programming Language Perspective</title>
</head>
<body>
<h1>Deep Learning, from a Programming Language Perspective</h1>
<p>
I bet you had heard of Artificial Neural Network(if no, go read it now), but specifically, what is back propagation and the intuition behind it?
I will try to explain concept in Deep Learning from a Programming Language perspective, and point out what we can get from doing so.
At the same time, I will also talk about the Deep Learning framework I developed, 
<a href="https://github.com/ThoughtWorksInc/DeepDarkFantasy/" target="_blank">DeepDarkFantasy(DDF)</a>, to show how we can leverage concept from PL.
</p>

<b>What is a neural network?</b>

<p>
In one sentence, A Neural Network is just a program with unknown parameters!
Those unknown parameters, are the weight of the neural network.
Forward propagation is evaluation of the program.
Deep Learning is the process of finding the best parameters.
</p>

<p>For example, let's say we have the following Neural Network(and assuming the activation function is relu):</p>

<p>Input0 -----Output0</p>

<p>Input1 -------/\</p>

</p>The scala code equivalent is:</p>
</p>
def nn(in0 : Double)(in1 : Double)(w0 : Double)(w1 : Double) = max(in0 * w0 + in1 * w1, 0)
</p>

<b>How to Train Your Neural Network</b>

<p>What should we do to find the best parameter?</p>
<p>
Obviously, we need to define 'best'. 
We can introduce a function that, given weights, use the weights to run the neural network, and give a score.
The best weights maximize/minimize the score.
The function is called Loss Function.
</p>

<p>Assuming neural network have only a weight, a double:</p>
<p>type Weight = Double</p>
<p>LossFunction is all function that take a weight and return a double</p>
<p>type LossFunction = Weight => Double</p>

<p>
A typical loss function is Mean Square Error: 
for every dimension of a multi-dimensional output by neural network and the expected output, square the difference.
Divide the result by the number of dimension.
For one-dimensional output, this is just the square of difference.
We can write it down as a scala function.
</p>

<p>def MSE(l: Seq[Double])(r: Seq[Double]) = (l.zip(r).map(p => p._1 * p._2).sum: Double) / l.length</p>
<p>
This have different type then the LossFunction defined above.
However, given a data set of the type Seq[(Input, Output)], we can turn it into a loss function:
given weight, for every (input, output) on the data set, run the neural network with weight and input.
Compare the output with the expected output.
Sum all the absolute difference.
</p>

<p>It is possible to transform a LossFunction, for example scaling it:</p>
<p>def Scale(d: Double)(lf: LossFunction): LossFunction = w => d * lf(w)</p>

<p>Switching between gradient descend or gradient ascend is Scale -1</p>

<p>More transformation - Adding two LossFunction:</p>
<p>def Plus(l: LossFunction)(r: LossFunction): LossFunction = w => l(w) + r(w)</p>

<p>There are other LossFunction, for example L1 L2 regularization, for preventing large weights:</p>
<p>def L1: LossFunction = w => if (w > 0) w else -w</p>
<p>def L2: LossFunction = w => w * w</p>

<p>To add L2 onto a existing LossFunction, we might first scale L2 with a hyperparameter to adjust the regularization rate, and add he original loss.</p>
<p>Back to our main point: the most common method to minimize the Loss Function, is:</p>
<ol start="0">
<li>Find some random value as inital weight</li>
<li>Calculate the derivative for the Loss(with multiple weight this is a gradient)</li>
<li>Reduce it if derivative is positive, increase it otherwise.(with multiple weight update to the backward direction of gradient)</li>
<li>Repeat till you feel like it</li>
</ol>

<p>
This is called neural network, the backbone for a ton of training algorithm in Deep Learning. 
Back Propagation, is simply the process of finding derivative and updating.
</p>

<b>Where are derivative from?</b>
<p>Sorting by the level of automation:</p>
<ol>
<li>Calculate and write them down manually <del>This is what PHD are for</del></li>
<li>
Manually write down the derivative of a single layer, and compose them.
Caffe is like this: in Caffe, a module is a layer of Neural Network(NN), so we have Convolution Layer, Fully Connected Layer(called InnerProduct).
Those layer needed to be written by programmers, but other can compose their own Neural Network out of those.
</li>
<li>
Provide a Domain Specific Language(DSL), and implement NN layer/other NN architecture with it. 
If everything in DSL is differentiable, then NN implementation is Natrually differentiable.
In this case, the Deep Learning had become a Programming Language!
This is Tensorflow style.</li>
</ol>

<p>Essentially the three are the same thing: we define a DSL, manually write down the derivative for primitive construct, and implement what we want with it.</p>
<p>
In caffe style, the primitive is composable(although not very flexible) layers. 
In the more extreme case of 100% manual, there are only one primitive (the whole algorithm), without any compositionality.
In this view, the largest divide in different framework/handcrafting is just the 'size' of basic operation.
The smaller the primitive construct is, the more flexible, (ignoring efficiency) the easier to implement NN.
The larger it is, the more efficient it is: optimizing small construct is hard.
</p>

<p>At the same time, there are three way to represent derivative, sorting by flexibility(again, the more flexible isnt necessarily better)</p>
<ol>
<li>For a program in DSL, return a derivative calculation function, outside of the DSL. Caffe.</li>
<li>
For a program in DSL, return a program in DSL that calculate derivative, so it is possible to do further optimization, or get higher order derivative.
Theano. DeepDarkFantasy.
</li>
<li>There is a function in DSL that do differentiation. StalinGrad.</li>
</ol>

<b>What is the features of DDF?</b>

<p>
Although DL framework is PL, they dont support much programming construct: 
there are basic operation on Double, some have condition/loop, some have scan/variable(see tensorflow whitepaper), and pretty much that's it.
DDF had add all sorts of ordinary PL construct, such as recursion, goto(continuation), exception, IO.
</p>

跟其他AD语言比起来，DDF的特点是，是一个Typed Extensible Embedded DSL - 这DSL造出的AST是scala中强类型的term，并且这个DSL可以很简单的在scala中扩展。同时，DDF是Typed的-我们可以给出任意type的导数类型，也可以对任意term求导（并且有正确的type）。我们也同时（未完成）给出了一个算法是正确的证明。

<b>How do DDF work?</b>

对于任意一个term，如果要找出他的导数形式，我们只需要把所有Double换成对应的二元数。但是，转换完以后，跟一般的，得出一个函数的二元数实现不同，得出的依然是一个AST-换句话说，AD其实可以是Symbolic的。这是DDF AD的原理。

换句话说，DDF抛弃了‘导数’这个概念。在DDF中，对一个东西求导以后，不会得出他的导数，只会得出该term跟该term的导数的一个混合物。打个比方：

Either[Double, (Double, Double)] => Either[Double, Double]并没有一个所谓的‘导数’。

但是可以把导数插进上面的类型，得出

Either[(Double, Double), ((Double, Double), (Double, Double))] => Either[(Double, Double), (Double, Double)]

这是上面类型的term，但是所有Double跟Double operation都转换成二元数的term，的类型。

注：的确可以给函数，Sum Type，找出单独的类型，早期DDF也是这样做的，但是我不喜欢，放弃了。

至于如何做Typed Extensible EDSL，可以看Finally Tagless

至于如何表示Lambda Abstraction，可以看Compiling Combinator

<b>Formal Definition</b>

可能这些东西都太玄乎，大家都没理解，于是我就给出一个缩小版的DDF，DDF-min，并更严谨地定义DDF-min，希望能帮助学过点Type Theory的人理解：

DDF-min基于Call By Value Simply Typed Lambda Calculus，带有Real，Sum Type, Product Type, Recursion（using Y schema）

有with_grad_t函数，可以traverse type structure，然后把所有遇到的Real转换成Real * Real。

还有with_grad函数，可以traverse AST，然后把类型转换成with_grad_t

然后有个logical relation，对于函数外的东西，都是trivial的定义，或者简单的recurse进去。

对于A -> B，除了普通的‘对所有符合logical relation的A，application满足logical relation’外，还有：如果A -> B = Real -> Real，这个函数的with_grad加点wrapper就是这个函数的Denotational Semantic的导数函数。

另：这根MarisaKirisame/DDFADC中描述的有一定出入。

Forward Mode AD会不会有性能问题？

如果对AD很熟的朋友，肯定会指出一个问题：如果有N个Double输入，Forward Mode AD就要运行N次。对于有着很多参数的神经网络来说，这无法忍受！

解决办法是，我们对Dual Number做一次Generalization：Dual Number并不一定是(Double, Double)，也可以是(Double, (Double, Double))。用后者，可以运行一次，算出两个导数。

比如说，给定x, y, z，并且想知道(x+y)*z对于x, z的导，

可以写出

((x, (1, 0)) + (y, (0, 0))) * (z, (0, 1)) =

(x + y,(1, 0)) * (z, (0, 1)) =

((x + y) * z, (z, x + y))

在这里面，pair的第0项就是表达式的值，pair的第1项就是另一个pair，其中第0,1，项分别是表达式对于x，y的导。

或者，可以用(Double, Double => Double[1000])（注：Double[1000]不是真正的scala代码）代替(Double, Double[1000])-这样，当整个term要乘以一个literal的时候，并不需要进入整个Array去算，只需要update该Double则可-这就是反向传播。

在实现中，这通过引入一个Typeclass，Gradient（满足的有Unit, (Double, Double),(Double => Double[1000])等），（并限制Gradient一定要满足field的一个variation（其实本质上还是一个field，只不过为了提速）），并用之于Dual Number之上（第二个参数不再是Double，而是该Gradient）。然后，AD的四则运算就可以利用Field的操作写出。

<b>So what is the main benefit from it, from a user point of view?</b>
这有什么用？

我们希望能做到Neural Networks, Types, and Functional Programming里面给的例子

DDF可以很简单的给出有递归/循环的函数的高阶导。这点tensorflow就不支持（Gradients of non-scalars (higher rank Jacobians) · Issue #675 · tensorflow/tensorflow）。

除了写神经网络以外，我们也希望可以写任意普通的算法，程序（但是带有未知变量），然后用DDF自动求导，以找出最优的这些变量。

能不能给个例子？这是一个用梯度下降解x*x+2x+3=27的例子。
</body>
</html>