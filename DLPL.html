<html>
<head>
<meta charset="utf-8" />
<title>Deep Learning, from a Programming Language Perspective</title>
</head>
<body>
<h1>Deep Learning, from a Programming Language Perspective</h1>
<p>
I bet you had heard of Artificial Neural Network(if no, go read it now), but specifically, what is back propagation and the intuition behind it?
I will try to explain concept in Deep Learning from a Programming Language perspective, and point out what we can get from doing so.
At the same time, I will also talk about the Deep Learning framework I developed, 
<a href="https://github.com/ThoughtWorksInc/DeepDarkFantasy/" target="_blank">DeepDarkFantasy(DDF)</a>, to show how we can leverage concept from PL.
</p>

<b>What is a neural network?</b>

<p>
In one sentence, A Neural Network is just a program with unknown parameters!
Those unknown parameters, are the weight of the neural network.
Forward propagation is evaluation of the program.
Deep Learning is the process of finding the best parameters.
</p>

<p>For example, let's say we have the following Neural Network(and assuming the activation function is relu):</p>

<p>Input0 -----Output0</p>

<p>Input1 -------/\</p>

</p>The scala code equivalent is:</p>
</p>
def nn(in0 : Double)(in1 : Double)(w0 : Double)(w1 : Double) = max(in0 * w0 + in1 * w1, 0)
</p>

<b>How to Train Your Neural Network</b>

<p>What should we do to find the best parameter?</p>
<p>
Obviously, we need to define 'best'. 
We can introduce a function that, given weights, use the weights to run the neural network, and give a score.
The best weights maximize/minimize the score.
The function is called Loss Function.
</p>

<p>Assuming neural network have only a weight, a double:</p>
<p>type Weight = Double</p>
<p>LossFunction is all function that take a weight and return a double</p>
<p>type LossFunction = Weight => Double</p>

<p>
A typical loss function is Mean Square Error: 
for every dimension of a multi-dimensional output by neural network and the expected output, square the difference.
Divide the result by the number of dimension.
For one-dimensional output, this is just the square of difference.
We can write it down as a scala function.
</p>

<p>def MSE(l: Seq[Double])(r: Seq[Double]) = (l.zip(r).map(p => p._1 * p._2).sum: Double) / l.length</p>
<p>
This have different type then the LossFunction defined above.
However, given a data set of the type Seq[(Input, Output)], we can turn it into a loss function:
given weight, for every (input, output) on the data set, run the neural network with weight and input.
Compare the output with the expected output.
Sum all the absolute difference.
</p>

<p>It is possible to transform a LossFunction, for example scaling it:</p>
<p>def Scale(d: Double)(lf: LossFunction): LossFunction = w => d * lf(w)</p>

<p>Switching between gradient descend or gradient ascend is Scale -1</p>

<p>More transformation - Adding two LossFunction:</p>
<p>def Plus(l: LossFunction)(r: LossFunction): LossFunction = w => l(w) + r(w)</p>

<p>There are other LossFunction, for example L1 L2 regularization, for preventing large weights:</p>
<p>def L1: LossFunction = w => if (w > 0) w else -w</p>
<p>def L2: LossFunction = w => w * w</p>

<p>To add L2 onto a existing LossFunction, we might first scale L2 with a hyperparameter to adjust the regularization rate, and add he original loss.</p>
<p>Back to our main point: the most common method to minimize the Loss Function, is:</p>
<ol start="0">
<li>Find some random value as inital weight</li>
<li>Calculate the derivative for the Loss(with multiple weight this is a gradient)</li>
<li>Reduce it if derivative is positive, increase it otherwise.(with multiple weight update to the backward direction of gradient)</li>
<li>Repeat till you feel like it</li>
</ol>

<p>
This is called neural network, the backbone for a ton of training algorithm in Deep Learning. 
Back Propagation, is simply the process of finding derivative and updating.
</p>

<b>Where are derivative from?</b>
<p>Sorting by the level of automation:</p>
<ol>
<li>Calculate and write them down manually <del>This is what PHD are for</del></li>
<li>
Manually write down the derivative of a single layer, and compose them.
Caffe is like this: in Caffe, a module is a layer of Neural Network(NN), so we have Convolution Layer, Fully Connected Layer(called InnerProduct).
Those layer needed to be written by programmers, but other can compose their own Neural Network out of those.
</li>
<li>
Provide a Domain Specific Language(DSL), and implement NN layer/other NN architecture with it. 
If everything in DSL is differentiable, then NN implementation is Natrually differentiable.
In this case, the Deep Learning had become a Programming Language!
This is Tensorflow style.</li>
</ol>

<p>Essentially the three are the same thing: we define a DSL, manually write down the derivative for primitive construct, and implement what we want with it.</p>
<p>
In caffe style, the primitive is composable(although not very flexible) layers. 
In the more extreme case of 100% manual, there are only one primitive (the whole algorithm), without any compositionality.
In this view, the largest divide in different framework/handcrafting is just the 'size' of basic operation.
The smaller the primitive construct is, the more flexible, (ignoring efficiency) the easier to implement NN.
The larger it is, the more efficient it is: optimizing small construct is hard.
</p>

<p>At the same time, there are three way to represent derivative, sorting by flexibility(again, the more flexible isnt necessarily better)</p>
<ol>
<li>For a program in DSL, return a derivative calculation function, outside of the DSL. Caffe.</li>
<li>
For a program in DSL, return a program in DSL that calculate derivative, so it is possible to do further optimization, or get higher order derivative.
Theano. DeepDarkFantasy.
</li>
<li>There is a function in DSL that do differentiation. StalinGrad.</li>
</ol>

<b>What is the features of DDF?</b>

<p>
Although DL framework is PL, they dont support much programming construct: 
there are basic operation on Double, some have condition/loop, some have scan/variable(see tensorflow whitepaper), and pretty much that's it.
DDF had add all sorts of ordinary PL construct, such as recursion, goto(continuation), exception, IO.
</p>

<p>
Comparing to other AD tool, DDF is a Typed Extensible Embedded DSL - 
AST of the DSL is richly-typed scala term, and it is very easy to extend the DSL in scala.
We also extend AD to work on others type and term (and return term with correct type), 
and (not completed yet) give a formal proof for it's correctness.
</p>

<b>How do DDF work?</b>

<p>
In order to find the derivative form for an arbitrary term, we just need to replace all occurence of Real with Dual Number.
However, we quote the whole code after transformation, so we have an AST afterward.
In another word, AD is symbolic.
</p>

<p>
As a consequence of this, DDF have abandoned the concept of a loss.
After calculating the derivative for a term, we will not have a loss, but only a hybrid of the original term and the loss.
</p>
<p>As an example, the gradient for "Either[Double, (Double, Double)] => Either[Double, Double]" does not exists.</p>
<p>Instead, we have a hybrid by inserting the gradient in it:</p>
<p>Either[(Double, Double), ((Double, Double), (Double, Double))] => Either[(Double, Double), (Double, Double)]</p>
<p>It is simply the original type but with every occurrences of Double replaced with pair of it.</p>

<p>
Note: It is infact possible to find the gradient for Sum type or Function, which is what early stage DDF do.
I just abandon the approach since I do not like it
</p>

<p>
As for how to do Typed Extensible EDSL, see
<a href="http://www.cs.cornell.edu/info/projects/nuprl/PRLSeminar/PRLSeminar2011/Chung-chiehShan-FinallyTaglessPartiallyEevaluated.pdf" target="_blank">Finally Tagless</a>
</p>

<p>
And for how to do Lambda Abstraction, see <a href="https://zhuanlan.zhihu.com/p/22231273" target="_blank">Compiling Combinator</a>
</p>

<b>Formal Definition</b>

<p>
Maybe all those are too much Hand-waving, so I will give a formal definition of mini DDF, DDF-min,
so it will help the type-theoretically inclined reader.
</p>

<p>
DDF-min is based on Call By Value Simply Typed Lambda Calculus, with Real, Sum, Product, and Recursion under Y Combinator Schema
Instead of using lambda abstraction, we represent stuff with SKI & BCKW Combinator instead.
Outside of the language, there's a with_grad_t function, which traverse the type structure, and replace Real with Real * Real.
Thre is a with_grad function, which traverse an AST, transforming it from AST of type t to type (with_grad_t t).
And there's a Logical Relation which, for everything except a Function, have a trivial definition - 
everything satisfy, or for recursive case, what made it up also satisfy the logical relation.
For f:A -> B, beside the standard requirement(for every x:A in the logical relation, f x:B is also in),
we also have: if A -> B = Real -> Real(which is A = Real /\ B = Real), the with_grad of this function, with some wrapper,
return the Newtonian definition of derivative on one dimensional calculus of f.
</p>

<p>Note: This is not completely the same with what MarisaKirisame/DDFADC describe.</p>

<b>Will Forward Mode AD cause efficiency trouble?</b>

<p>
If you are familiar with AD, you will point out that, if there are N Double input, Forward Mode must activate N time.
This is simply unacceptable.
Luckily, this can be solved by generalizing Dual Number: It need not be of type Double * Double.
It can be Double * (Double * Douhle), and using the latter, we can calculate two derivative in one pass.
For example, given x, y, z, and if we want to get the derivative of (x + y) * z with respect to x and z,
we can write:
</p>

<p>((x, (1, 0)) + (y, (0, 0))) * (z, (0, 1)) =</p>
<p>(x + y,(1, 0)) * (z, (0, 1)) =</p>
<p>((x + y) * z, (z, x + y))</p>

<p>
In the above equation, the zeroth term of the pair is the value for the original expression.
The first term of the pair is another pair, with 0, 1 term being derivative for x, y.
</p>

<p>
Or, we can replace (Double * Double[1000]) with (Double * Double -> Double[1000]),
so we do not have to traverse the whole array when we multiply a term with a literal - we simply update the double passed in.
This is Reversed Mode Automatic Differentiation.
</p>

<p>
In DDF Implementation, we achieve this by introducing a TypeClass, Gradient, 
which is satisfy by Unit, Double Double * Double, (Double => Double[1000]), and such, with constraint very similar to a Field.
We use it to define Dual Number of the type Double * Gradient, and we can write fundamental operation using construct on Field.
</p>

<b>So what is the main benefit from it, from a user point of view?</b>

<p>
We want to achieve the examples given by
<a href="http://colah.github.io/posts/2015-09-NN-Types-FP/" target="_blank">Neural Networks, Types, and Functional Programming</a>
</p>

<p>
We can give higher order derivative for code involving loop/recursion. 
<a href="https://github.com/tensorflow/tensorflow/issues/675" target="_blank">Tensorflow</a> do not support such.
</p>

<p>
We also want to be able to write arbitrary normal algorithm/program, but with unknown weight, 
and use DDF to find the best value for the weight
</p>

<p>And at last, <a href="https://github.com/ThoughtWorksInc/DeepDarkFantasy/blob/master/doc/poly.md" target="_blank">here</a> is an example of using gradient descent to solve x*x+2x+3=27.</p>

<div id="disqus_thread"></div>
<script type="text/javascript">
	var disqus_shortname = 'marisa-moe';
	(function() {
		var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
		dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
	})();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
</body>
</html>